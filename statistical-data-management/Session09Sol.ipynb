{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Statistical Data Management Session 9: Inferences Based on a Single Sample Tests of Hypothesis (chapter 8 in McClave & Sincich)\n",
    "\n",
    "## 1. The European ℮ Standard\n",
    "\n",
    "Prepackaged items in the EU may bear the ℮-mark to show that they are conforming with EU weight standards (see  https://europa.eu/youreurope/business/product-requirements/labels-markings/emark/index_en.htm). \n",
    "\n",
    "1. To test the claim of your favourite crisp brand that their packages contain 120g, you weigh the contents of 20 packages and find $\\bar{x} = 119.5$ and $s=0.8$. Is this brand complying to EU regulations correctly? The Council Directive of 20 January 1976 \"on the approximation of the laws of the Member States relating to the making-up by weight or by volume of certain prepackaged products\" OJ L 046 21.2.1976, p. 1 stipulates a one-sided t-test at confidence level $\\alpha = 0.005$. You may assume the weights follow a normal distribution.\n",
    "\n",
    "    $H_0: \\mu = 120$, we test whether the sample weighs less than stipulated, so perform a one-sided test: $H_a: \\mu < 120$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats as sts\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "n = 20\n",
    "x_bar = 119.5\n",
    "s = 0.8\n",
    "mu_0 = 120\n",
    "\n",
    "alpha = 0.005\n",
    "t_distribution = sts.t(n - 1)\n",
    "t_alpha = t_distribution.ppf(alpha)\n",
    "\n",
    "t = np.sqrt(n) * (x_bar - mu_0) / s\n",
    "\n",
    "print(\"Critical t value:\", t_alpha)\n",
    "print(\"Obtained t value:\", t)\n",
    "# The t-value is not smaller than t_alpha, so don't reject H_0.\n",
    "# You cannot claim that the packages are not filled enough according to EU regulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Calculate and interpret the $p$-value of the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = t_distribution.cdf(t)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the probability $p$ that, if $H_0$ is true, a sample of size 20 has a mean $\\bar{x} = 119.5$ or less. Note that $p>\\alpha$, which we expected, as the test was not significant. Or, in other words, the probability that the observed deficiency (less than the required 120) in weight can be attributed to chance, is higher than the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now assume you have more information about the production process, implying that we know that the machines that fill the packages do this with a standard deviation $\\sigma = 0.8$. Perform the test again!\n",
    "\n",
    "    $\\sigma$ is known + assumption of normality, so you may perform a large-sample test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "x_bar = 119.5\n",
    "sigma = 0.8\n",
    "mu_0 = 120\n",
    "\n",
    "alpha = 0.005\n",
    "standard_normal = sts.norm(0,1)\n",
    "z_alpha = standard_normal.ppf(alpha)\n",
    "\n",
    "z = np.sqrt(n) * (x_bar - mu_0) / sigma\n",
    "\n",
    "print(\"Critical z value:\", z_alpha)\n",
    "print(\"Obtained z value:\", z)\n",
    "# Now the z-value is smaller than z_alpha, so reject H_0 in favour of H_a.\n",
    "# Note that knowledge of this population parameter, rather than having to estimate it as well from the sample,\n",
    "# leads to (potentially) stronger conclusions!\n",
    "\n",
    "# Same with p-value:\n",
    "print(standard_normal.cdf(z))\n",
    "# p-value < alpha => significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparing Exam UML Tasks\n",
    "\n",
    "Say that for an *Object Oriented Software Development* exam, it is known that an expected proportion of 75% of participants passes the UML modelling task. In the interest of fairness, the teachers of this course want to guard whether different exams are comparable. In the file ``uml.csv`` in the ``shared`` folder, you find scores, out of 64, of an exam UML taks. Run the cell below to define the proportion of passed students for this exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uml = pd.read_csv(\"../../shared/uml.csv\")\n",
    "n = len(df_uml)\n",
    "p_hat = len(df_uml[df_uml[\"MarksUML\"] >= 32]) / n\n",
    "print(\"Number of participants:            \", n)\n",
    "print(\"Proportion of students who passed: \", p_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a test at significance level $\\alpha = 0.05$ to check whether this pass-rate is significantly different from $75\\%$.\n",
    "\n",
    "$H_0: p = 0.75$, we test whether results are different so perform a two-sided test: $H_a: p \\neq 0.75$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_hat = 1 - p_hat\n",
    "print(\"Large sample test OK? =>\", n * q_hat > 15 and n * p_hat > 15)\n",
    "p_0 = 0.75\n",
    "q_0 = 1 - p_0\n",
    "\n",
    "alpha = 0.05\n",
    "standard_normal = sts.norm(0,1)\n",
    "z_alpha = standard_normal.ppf(1 - alpha/2) # two-sided: \"divide uncertainty over two tails\"\n",
    "\n",
    "z = np.sqrt(n) * (p_hat - p_0) / np.sqrt(p_0 * q_0)\n",
    "print(\"Critical z values:\", -z_alpha, z_alpha)\n",
    "print(\"Obtained z value:\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $-z_{\\alpha/2} < z < z_{\\alpha/2}$, we do not reject $H_0$ and conclude that the pass-rate is not significantly different from 0.75 at level of significance $\\alpha = 0.05$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. List Performance\n",
    "\n",
    "How long does it take Python to do an operation on a huge list? Repeated simulations lead me to hazard the opinion that the function ``fill_with_ones()`` defined below takes 0.025 seconds to run in my Notebook interpreter, when applied to a list of a million items. We will test whether your hub performs worse (i.e. longer execution time).\n",
    "\n",
    "1. To check this, we need a data set. One execution of a function is not representative as the execution time depends on other processes as well. To overcome this problem, the code below executes the function call to ``fill_with_ones()`` 100 times. Run the code to generate your data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_ones(array): #      silly function that simply overwrites all entries in an array with ones\n",
    "    for i in range(len(array)):\n",
    "        array[i] = 1\n",
    "\n",
    "dummy_array = [0]*1000000 #       define an array with a million zeroes\n",
    "times = np.empty(100) #           array to catch the time it takes for 100 simulations\n",
    "\n",
    "for i in range(100): #            do this a 100 times\n",
    "    start = time.time() #         log the time now, before the function call\n",
    "    fill_with_ones(dummy_array) # call the function\n",
    "    end = time.time() #           log the time again, after the function call\n",
    "    print(end - start) #          print the time difference\n",
    "    times[i] = end - start #      save the time difference\n",
    "print(\"Mean:\", times.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Formulate $H_0$ and $H_a$.\n",
    "\n",
    "    $H_0: \\mu = 0.025$, we test whether your machine performs worse, so perform a one-sided test: $H_a: \\mu > 0.025$.\n",
    "\n",
    "3. Perform the test at significance level $\\alpha = 0.01$.\n",
    "\n",
    "    $z_\\alpha = 2.3263$ (calculated below).\n",
    "    $z = 10(\\bar{x} - 0.025)/s$, with your sample statistics. If $z > z_\\alpha$, reject $H_0$ in favour of $H_a$, else don't reject $H_0$, at 0.01 significance.\n",
    "\n",
    "    In Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_normal = sts.norm(0,1)\n",
    "z_alpha = standard_normal.ppf(1 - 0.01)\n",
    "n = 100\n",
    "x_bar = times.mean()\n",
    "\n",
    "s = times.std()\n",
    "mu_0 = 0.025\n",
    "\n",
    "z = np.sqrt(n) * (x_bar - mu_0) / s\n",
    "\n",
    "print(\"Critical z value:\", z_alpha)\n",
    "print(\"Obtained z value:\", z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Birth Weight\n",
    "\n",
    "In last week's exercises, we calculated a $90\\%$ confidence interval, based on $n=42$ and $\\bar{x}=3.31$, for babies' birthweight: $[3.16, 3.47]$. Assume that these data were obtained from a sample in one hospital. We want to test, at $\\alpha=0.05$, whether the weight of babies born in this hospital is significantly less than the national average, which is $\\mu = 3.4$ kg.\n",
    "\n",
    "1. Comment on the following reasoning: \"$3.4$ lies more to the right ($3.4>3.31$) in this interval, so the birth weight in this hospital is indeed significantly less than in the national population.\"\n",
    "\n",
    "    This is a wrong conclusion: while the average in this sample is less than the population mean, we don't know whether it is **significantly** less, i.e. if we are (at a certain level of certainty) no longer willing to attribute the observed difference to chance.\n",
    "    \n",
    "2. Draw the correct conclusion based on the given confidence interval.\n",
    "\n",
    "    $H_0: \\mu=3.4$, we test whether the observed weight is significantly less, so perform a one-sided test: $H_a: \\mu<3.4$. \n",
    "    \n",
    "    As $\\alpha=0.05$, for a two-sided test, a $90\\%$ confidence interval solves the question!\n",
    "    \n",
    "    Conclusion: as $3.4 \\in [3.16, 3.47]$, we cannot reject $H_0$, the observed difference is not significant at $\\alpha=0.05$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SQL Recap\n",
    "\n",
    "The file ``uml.sql`` provided on Toledo contains the information used in exercise 2: student q-numbers and scores. Note that certain students occur twice, e.g. q-number 114 with scores 14 and 15. In that case, their answer was spread over multiple pages and their score is the sum of these individual numbers. Import the file using MySQL Workbench and write the appropriate queries to retrieve the relevant information. Re-run your analysis (without running the cell which defined the dataframe!) to check whether you have the correct information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"../../shared/uml.db\")\n",
    "\n",
    "# should generate a single result, namely the number of students\n",
    "query_total = \"\"\"\n",
    "SELECT COUNT(DISTINCT q_number) AS total FROM marks\n",
    "\"\"\"\n",
    "\n",
    "# should generate all students (q-numbers) who passed\n",
    "query_passed = \"\"\"\n",
    "SELECT COUNT(DISTINCT q_number) AS passed FROM marks WHERE q_number IN (SELECT q_number FROM marks GROUP BY q_number HAVING SUM(MarksUML) >=32)\n",
    "\"\"\"\n",
    "\n",
    "df_total = pd.read_sql_query(query_total, conn)\n",
    "df_passed = pd.read_sql_query(query_passed, conn)\n",
    "print(df_total)\n",
    "print(df_passed)\n",
    "\n",
    "# Note that our analysis still works!\n",
    "n = df_total[\"total\"]\n",
    "p_hat = df_passed[\"passed\"] / n\n",
    "\n",
    "q_hat = 1 - p_hat\n",
    "\n",
    "p_0 = 0.75\n",
    "q_0 = 1 - p_0\n",
    "\n",
    "alpha = 0.05\n",
    "standard_normal = sts.norm(0,1)\n",
    "z_alpha = standard_normal.ppf(1 - alpha/2) # two-sided: \"divide uncertainty over two tails\"\n",
    "\n",
    "z = np.sqrt(n) * (p_hat - p_0) / np.sqrt(p_0 * q_0)\n",
    "print(\"Critical z values:\", -z_alpha, z_alpha)\n",
    "print(\"Obtained z value:\", z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
